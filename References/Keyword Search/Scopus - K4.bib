Scopus
EXPORT DATE: 15 July 2022

@ARTICLE{Saghiri2022,
author={Saghiri, A.M. and Vahidipour, S.M. and Jabbarpour, M.R. and Sookhak, M. and Forestiero, A.},
title={A Survey of Artificial Intelligence Challenges: Analyzing the Definitions, Relationships, and Evolutions},
journal={Applied Sciences (Switzerland)},
year={2022},
volume={12},
number={8},
doi={10.3390/app12084054},
art_number={4054},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129164021&doi=10.3390%2fapp12084054&partnerID=40&md5=ccc45bee6111a2cbab2cb5bbbf96a7d0},
abstract={In recent years, artificial intelligence has had a tremendous impact on every field, and several definitions of its different types have been provided. In the literature, most articles focus on the extraordinary capabilities of artificial intelligence. Recently, some challenges such as security, safety, fairness, robustness, and energy consumption have been reported during the development of intelligent systems. As the usage of intelligent systems increases, the number of new challenges increases. Obviously, during the evolution of artificial narrow intelligence to artificial super intelligence, the viewpoint on the challenges such as security will be changed. In addition, the recent development of human-level intelligence cannot appropriately happen without considering whole challenges in designing intelligent systems. Considering the mentioned situation, no study in the literature summarizes the challenges in designing artificial intelligence. In this paper, a review of the challenges is presented. Then, some important research questions about the future dynamism of challenges and their relationships are answered. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={artificial general intelligence;  artificial intelligence;  artificial narrow intelligence;  artificial superintelligence;  challenges;  human-level intelligence},
publisher={MDPI},
issn={20763417},
language={English},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Dhanabalachandran202125,
author={Dhanabalachandran, K. and Hassouna, V. and Hedblom, M.M. and Küempel, M. and Leusmann, N. and Beetz, M.},
title={Cutting Events: Towards Autonomous Plan Adaption by Robotic Agents through Image-Schematic Event Segmentation},
journal={K-CAP 2021 - Proceedings of the 11th Knowledge Capture Conference},
year={2021},
pages={25-32},
doi={10.1145/3460210.3493585},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120897761&doi=10.1145%2f3460210.3493585&partnerID=40&md5=132974929e4a602bcc224bb9405fddf1},
abstract={Autonomous robots struggle with plan adaption in uncertain and changing environments. Although modern robots can make popcorn and pancakes, they are incapable of performing such tasks in unknown settings and unable to adapt action plans if ingredients or tools are missing. Humans are continuously aware of their surroundings. For robotic agents, real-time state updating is time-consuming and other methods for failure handling are required. Taking inspiration from human cognition, we propose a plan adaption method based on event segmentation of the image-schematic states of subtasks within action descriptors. For this, we reuse action plans of the robotic architecture CRAM and ontologically model the involved objects and image-schematic states of the action descriptor cutting. Our evaluation uses a robot simulation of the task of cutting bread and demonstrates that the system can reason about possible solutions to unexpected failures regarding tool use. © 2021 Owner/Author.},
author_keywords={autonomous agents;  cognitive robotics;  event segmentation;  image schemas;  plan adaption;  situational assessment},
publisher={Association for Computing Machinery, Inc},
isbn={9781450384575},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hassanin2021,
author={Hassanin, M. and Khan, S. and Tahtali, M.},
title={Visual Affordance and Function Understanding},
journal={ACM Computing Surveys},
year={2021},
volume={54},
number={3},
doi={10.1145/3446370},
art_number={3446370},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108101164&doi=10.1145%2f3446370&partnerID=40&md5=afbc4c3021f06453b0547a46c3ac0bb2},
abstract={Nowadays, robots are dominating the manufacturing, entertainment, and healthcare industries. Robot vision aims to equip robots with the capabilities to discover information, understand it, and interact with the environment, which require an agent to effectively understand object affordances and functions in complex visual domains. In this literature survey, first, "visual affordances"are focused on and current state-of-The-Art approaches for solving relevant problems as well as open problems and research gaps are summarized. Then, sub-problems, such as affordance detection, categorization, segmentation, and high-level affordance reasoning, are specifically discussed. Furthermore, functional scene understanding and its prevalent descriptors used in the literature are covered. This survey also provides the necessary background to the problem, sheds light on its significance, and highlights the existing challenges for affordance and functionality learning. © 2021 ACM.},
author_keywords={Affordance prediction;  deep learning;  functional scene understanding;  visual reasoning},
publisher={Association for Computing Machinery},
issn={03600300},
coden={ACSUE},
language={English},
document_type={Review},
source={Scopus},
}

@ARTICLE{Pineda20211,
author={Pineda, L.A. and Hernández, N. and Rodríguez, A. and Cruz, R. and Fuentes, G.},
title={Deliberative and conceptual inference in service robots},
journal={Applied Sciences (Switzerland)},
year={2021},
volume={11},
number={4},
pages={1-37},
doi={10.3390/app11041523},
art_number={1523},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100854091&doi=10.3390%2fapp11041523&partnerID=40&md5=9e37197e60efb555ca34c4ab3fb29e89},
abstract={Service robots need to reason to support people in daily life situations. Reasoning is an expensive resource that should be used on demand whenever the expectations of the robot do not match the situation of the world and the execution of the task is broken down; in such scenarios, the robot must perform the common sense daily life inference cycle consisting on diagnosing what happened, deciding what to do about it, and inducing and executing a plan, recurring in such behavior until the service task can be resumed. Here, we examine two strategies to implement this cycle: (1) a pipe-line strategy involving abduction, decision-making, and planning, which we call deliberative inference and (2) the use of the knowledge and preferences stored in the robot’s knowledge-base, which we call conceptual inference. The former involves an explicit definition of a problem-space that is explored through heuristic search, and the latter is based on conceptual knowledge, including the human user preferences, and its representation requires a non-monotonic knowledge-based system. We compare the strengths and limitations of both approaches. We also describe a service robot conceptual model and architecture capable of supporting the daily life inference cycle during the execution of a robotics service task. The model is centered in the declarative specification and interpretation of robot’s communication and task structure. We also show the implementation of this framework in the fully autonomous robot Golem-III. The framework is illustrated with two demonstration scenarios. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={And planning in service robots;  Conceptual model of service robots;  Decisionmaking;  Declarative specification and interpretation of task and communication structure;  Diagnosis;  Non-monotonic reasoning;  Reasoning with preferences in service robots;  Sitlog;  Symbolic inference in service robots},
publisher={MDPI AG},
issn={20763417},
language={English},
document_type={Article},
source={Scopus},
}

@BOOK{Fermüller2021373,
author={Fermüller, C. and Maynord, M.},
title={Learning for action-based scene understanding},
journal={Advanced Methods and Deep Learning in Computer Vision},
year={2021},
pages={373-403},
doi={10.1016/B978-0-12-822109-9.00020-5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131501609&doi=10.1016%2fB978-0-12-822109-9.00020-5&partnerID=40&md5=6b1a0f126b32517dcf0f76415fe18897},
abstract={Action plays a central role in our lives and environments, yet most Computer Vision methods do not explicitly model action. In this chapter we outline an action-centric framework which spans multiple time scales and levels of abstraction, producing both action and scene interpretations constrained towards action consistency. At the lower level of the visual hierarchy we detail affordances - object characteristics which afford themselves to different actions. At mid-levels we model individual actions, and at higher levels we model activities through leveraging knowledge and longer term temporal relations. We emphasize the use of grasp characteristics, geometry, ontologies, and physics based constraints for generalizing to new scenes. Such explicit representations avoid overtraining on appearance characteristics. To integrate signal based perception with symbolic knowledge we align vectorized knowledge with visual features. We finish with a discussion on action and activity understanding, and discuss implications for future work. © 2022 Elsevier Inc. All rights reserved.},
author_keywords={Action-based representations;  Actions;  Activities;  Affordances;  Vector space embeddings},
publisher={Elsevier},
isbn={9780128221099; 9780128221495},
language={English},
document_type={Book Chapter},
source={Scopus},
}

@CONFERENCE{Pomarlan2021106,
author={Pomarlan, M. and Hedblom, M.M. and Porzel, R.},
title={Panta Rhei: Curiosity-Driven Exploration to Learn the Image-Schematic Affordances of Pouring Liquids},
journal={CEUR Workshop Proceedings},
year={2021},
volume={3105},
pages={106-117},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126583305&partnerID=40&md5=e777c9746827655f9afdf64e57f5eb24},
abstract={Despite rapid progress, cognitive robots have yet to match the facility with which humans acquire and find ways to reuse manipulation skills. An important component of human cognition seems to be our curiosity-driven exploration of our environments, which results in generalizable theories for action outcome prediction via analogical reasoning. In this paper, we implement a method to emulate this curiosity drive in simulations of the situation of pouring liquids between containers, and to use these simulations to construct a symbolic theory of pouring. The theory links qualitative descriptions of an initial state and manner of pouring with observed behaviors, and can be used to predict qualitative outcomes or select manners of pouring towards achieving a goal. © 2021 CEUR-WS. All rights reserved.},
author_keywords={Cognitive Robotics;  Commonsense Reasoning;  Curiosity-driven Learning;  Hybrid Approaches;  Intrinsic Motivation},
editor={Pakrashi A., Pakrashi A., Pakrashi A., Rushe E., Rushe E., Bazargani M.H.Z., Bazargani M.H.Z., Namee B.M., Namee B.M., Namee B.M., Namee B.M.},
publisher={CEUR-WS},
issn={16130073},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Toschev2020811,
author={Toschev, A. and Talanov, M.},
title={Artificial Cognitive Architectures Review},
journal={BioNanoScience},
year={2020},
volume={10},
number={4},
pages={811-823},
doi={10.1007/s12668-020-00768-4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088293255&doi=10.1007%2fs12668-020-00768-4&partnerID=40&md5=3f0d481d55a73edfaa5ce7ad071640fc},
abstract={In this work we present the review of cognitive architectures and bio-inspired approaches used for cognitive modeling with focus on consciousness and common sense computational implementation. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Cognitive modeling;  Common sense;  Consciousness;  Models},
publisher={Springer},
issn={21911630},
language={English},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Pfau2020335,
author={Pfau, J. and Malaka, R.},
title={We asked 100 people: How would you train our robot?},
journal={CHI PLAY 2020 - Extended Abstracts of the 2020 Annual Symposium on Computer-Human Interaction in Play},
year={2020},
pages={335-339},
doi={10.1145/3383668.3419864},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096757336&doi=10.1145%2f3383668.3419864&partnerID=40&md5=ec8d3436d8481c3283ae983fcceef3e5},
abstract={While robotic proficiency excels in constrained environments, the demand for vast amounts of world knowledge to cover unforeseen circumstances, constellations and tasks prevents sufficiently robust real-world application. Human computation has shown to provide successful advances to close this reasoning gap and accumulate knowledge, yet being greatly reliant on the quality of the provided data. In this paper, we introduce the game with a purpose Tool Feud that collects popularity rankings of object choices for robotic everyday activity tasks and evaluate an approach for classifying malicious responses automatically. © 2020 ACM.},
author_keywords={Everyday activities;  Games with a purpose;  Human computation},
publisher={Association for Computing Machinery, Inc},
isbn={9781450375870},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Barrocas2020,
author={Barrocas, R. and Roesch, S. and Gawrilow, C. and Moeller, K.},
title={Putting a Finger on Numerical Development – Reviewing the Contributions of Kindergarten Finger Gnosis and Fine Motor Skills to Numerical Abilities},
journal={Frontiers in Psychology},
year={2020},
volume={11},
doi={10.3389/fpsyg.2020.01012},
art_number={1012},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086152508&doi=10.3389%2ffpsyg.2020.01012&partnerID=40&md5=3a6d9975182a904799cfbd5c852ffb65},
abstract={The well-documented association between fingers and numbers is not only based on the observation that most children use their fingers for counting and initial calculation, but also on extensive behavioral and neuro-functional evidence. In this article, we critically review developmental studies evaluating the association between finger sensorimotor skills (i.e., finger gnosis and fine motor skills) and numerical abilities. In sum, reviewed studies were found to provide evidential value and indicated that both finger gnosis and fine motor skills predict measures of counting, number system knowledge, number magnitude processing, and calculation ability. Therefore, specific and unique contributions of both finger gnosis and fine motor skills to the development of numerical skills seem to be substantiated. Through critical consideration of the reviewed evidence, we suggest that the association of finger gnosis and fine motor skills with numerical abilities may emerge from a combination of functional and redeployment mechanisms, in which the early use of finger-based numerical strategies during childhood might be the developmental process by which number representations become intertwined with the finger sensorimotor system, which carries an innate predisposition for said association to unfold. Further research is nonetheless necessary to clarify the causal mechanisms underlying this association. © Copyright © 2020 Barrocas, Roesch, Gawrilow and Moeller.},
author_keywords={embodied numerosity;  fine motor skills;  finger counting;  finger gnosis;  finger-based numerical strategies;  mathematics achievement;  numerical development},
publisher={Frontiers Media S.A.},
issn={16641078},
language={English},
document_type={Review},
source={Scopus},
}

@ARTICLE{Schranz2020,
author={Schranz, M. and Umlauft, M. and Sende, M. and Elmenreich, W.},
title={Swarm Robotic Behaviors and Current Applications},
journal={Frontiers in Robotics and AI},
year={2020},
volume={7},
doi={10.3389/frobt.2020.00036},
art_number={36},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083512203&doi=10.3389%2ffrobt.2020.00036&partnerID=40&md5=85f2352eed091918b5712a5548459498},
abstract={In swarm robotics multiple robots collectively solve problems by forming advantageous structures and behaviors similar to the ones observed in natural systems, such as swarms of bees, birds, or fish. However, the step to industrial applications has not yet been made successfully. Literature is light on real-world swarm applications that apply actual swarm algorithms. Typically, only parts of swarm algorithms are used which we refer to as basic swarm behaviors. In this paper we collect and categorize these behaviors into spatial organization, navigation, decision making, and miscellaneous. This taxonomy is then applied to categorize a number of existing swarm robotic applications from research and industrial domains. Along with the classification, we give a comprehensive overview of research platforms that can be used for testing and evaluating swarm behavior, systems that are already on the market, and projects that target a specific market. Results from this survey show that swarm robotic applications are still rare today. Many industrial projects still rely on centralized control, and even though a solution with multiple robots is employed, the principal idea of swarm robotics of distributed decision making is neglected. We identified mainly following reasons: First of all, swarm behavior emerging from local interactions is hard to predict and a proof of its eligibility for applications in an industrial context is difficult to provide. Second, current communication architectures often do not match requirements for swarm communication, which often leads to a system with a centralized communication infrastructure. Finally, testing swarms for real industrial applications is an issue, since deployment in a productive environment is typically too risky and simulations of a target system may not be sufficiently accurate. In contrast, the research platforms present a means for transforming swarm robotics solutions from theory to prototype industrial systems. © Copyright © 2020 Schranz, Umlauft, Sende and Elmenreich.},
author_keywords={cyber-physical systems;  swarm behavior;  swarm intelligence;  swarm robotic applications;  swarm robotics},
publisher={Frontiers Media S.A.},
issn={22969144},
language={English},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Saghiri2020165,
author={Saghiri, A.M.},
title={A Survey on Challenges in Designing Cognitive Engines},
journal={2020 6th International Conference on Web Research, ICWR 2020},
year={2020},
pages={165-171},
doi={10.1109/ICWR49608.2020.9122273},
art_number={9122273},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089188983&doi=10.1109%2fICWR49608.2020.9122273&partnerID=40&md5=8ec4fbd539c0e01e756ea851f3b0447c},
abstract={The primary goal of cognitive computing is to design a digitalized model that is able to mimic human thinking processes. The cognitive engine is in charge of implementing the functionality of a cognitive system. Nowadays, cognitive engines are used as a self-organized management mechanism in different fields such as computer networks, Internet of Things (IoT), and Robotics. This is because the management algorithms of these fields are going to be very complex and therefore human thinking models as digitalized models are required for fast and accurate decision making. In this paper, we summarize challenges in designing cognitive engines. Then, a set of challenges in designing the cognitive engine for body-mind operating system in the digitalized healthcare system is obtained. In the literature, our survey and also suggested case study in the healthcare system have not been considered yet. © 2020 IEEE.},
author_keywords={Body-Mind Operating System;  Cognitive Engine;  Cognitive Systems;  Digitalized Healthcare},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781728110516},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Meywerk2020326,
author={Meywerk, T. and Walter, M. and Herdt, V. and Kleinekathöfer, J. and Große, D. and Drechsler, R.},
title={Verifying Safety Properties of Robotic Plans Operating in Real-World Environments via Logic-Based Environment Modeling},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2020},
volume={12478 LNCS},
pages={326-347},
doi={10.1007/978-3-030-61467-6_21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096489234&doi=10.1007%2f978-3-030-61467-6_21&partnerID=40&md5=d46b736ce898dfc8c0e0efef9e91d413},
abstract={These days, robotic agents are finding their way into the personal environment of many people. With robotic vacuum cleaners commercially available already, comprehensive cognition-enabled agents assisting around the house autonomously are a highly relevant research topic. To execute these kinds of tasks in constantly changing environments, complex goal-driven control programs, so-called plans, are required. They incorporate perception, manipulation, and navigation capabilities among others. As with all technological innovation, consequently, safety and correctness concerns arise. In this paper, we present a methodology for the verification of safety properties of robotic plans in household environments by a combination of environment reasoning using Discrete Event Calculus (DEC) and Symbolic Execution for effectively handling symbolic input variables (e. g. object positions). We demonstrate the applicability of our approach in an experimental evaluation by verifying safety properties of robotic plans controlling a two-armed, human-sized household robot packing and unpacking a shelf. Our experiments demonstrate our approach’s capability to verify several robotic plans in a realistic, logically formalized environment. © 2020, Springer Nature Switzerland AG.},
author_keywords={Cognition-Enabled Robotics;  Discrete Event Calculus;  Formal Verification;  Household Robots;  Symbolic Execution},
editor={Margaria T., Steffen B.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={03029743},
isbn={9783030614669},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kümpel2020,
author={Kümpel, M. and de Groot, A. and Tiddi, I. and Beetz, M.},
title={Using linked data to help robots understand product-related actions},
journal={CEUR Workshop Proceedings},
year={2020},
volume={2708},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095597558&partnerID=40&md5=d79398787e801955f8093fb5867df1e4},
abstract={Household robots need semantics to understand that a detergent is a cleaning product that can be used to clean physical objects like a table, but laundry detergent is only used to clean/wash laundry. A safely acting autonomous robot should also know that both will not be used as ingredients for meal preparation. We propose a new approach to connect robot sensor data to Linked Data in order to give robotic agents semantic product information about objects that can be found in their environment so that the action to be performed with a given object can be inferred. For this, we use the robot’s belief state when recognizing a product and link it to a product ontology that follows Semantic Web standards. We then use the product class information to fetch further information from external sources like Wikidata or ConceptNet that contain action information (e.g. laundry detergent is used for laundering). At last, the action results are mapped to internally known actions of the robotic agent so that it knows which action can be performed with the perceived object. Copyright © 2020 for this paper by its authors.},
author_keywords={Knowledge acquisition;  Knowledge graph;  Knowledge representation;  Linked data;  Product ontology},
editor={Hammar K., Kutz O., Dimou A., Hahmann T., Hoehndorf R., Masolo C., Vita R.},
publisher={CEUR-WS},
issn={16130073},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gouidis2020,
author={Gouidis, F. and Vassiliades, A. and Patkos, T. and Argyros, A. and Bassiliades, N. and Plexousakis, D.},
title={A Review on Intelligent Object Perception Methods Combining Knowledge-based Reasoning and Machine Learning∗},
journal={CEUR Workshop Proceedings},
year={2020},
volume={2600},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085202940&partnerID=40&md5=5f5e7694ceea2a0345fc807988ae5eb2},
abstract={Object perception is a fundamental sub-field of Computer Vision, covering a multitude of individual areas and having contributed high-impact results. While Machine Learning has been traditionally applied to address related problems, recent studies also seek ways to integrate knowledge engineering in order to expand the level of intelligence of the visual interpretation of objects, their properties and their relations with the environment. In this paper, we attempt a systematic investigation of how knowledge-based methods contribute to diverse object perception tasks. We review the latest achievements and identify prominent research directions. Copyright © 2020 held by the author(s).},
editor={Martin A., Hinkelmann K., Fill H.-G., Gerber A., Lenat D., Stolle R., van Harmelen F.},
publisher={CEUR-WS},
issn={16130073},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Nasir20192163,
author={Nasir, J. and Kim, D.-H. and Kim, J.-H.},
title={ART neural network-based integration of episodic memory and semantic memory for task planning for robots},
journal={Autonomous Robots},
year={2019},
volume={43},
number={8},
pages={2163-2182},
doi={10.1007/s10514-019-09868-x},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067284436&doi=10.1007%2fs10514-019-09868-x&partnerID=40&md5=e77f30173568bf30b4b12ba191fe55de},
abstract={Automated task planning for robots faces great challenges in that the sequences of events needed for a particular task are mostly required to be hard-coded. This can be a cumbersome process, especially, when the user wants a robot to learn a large number of similar tasks with different objects that are semantically related. We propose a novel approach of user preference-based integrated multi-memory model (pMM-ART). This approach focuses on exploiting a semantic hierarchy of objects alongside an episodic memory for enhancing the behavior of an autonomous agent. We analyze the functioning principle of the proposed model by teaching it a few distinct domestic tasks and observe that it is able to carry out a large number of similar tasks based on the semantic similarities between learned objects. We also demonstrate, via experiments using Mybot, our ability to reach those goals that are not possible without the integration of semantic knowledge with episodic memory. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Adaptive resonance theory;  Cognition;  Episodic memory;  Semantic memory;  Task planning;  User preference},
publisher={Springer New York LLC},
issn={09295593},
coden={AUROF},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chen2019,
author={Chen, H. and Luo, X.},
title={An automatic literature knowledge graph and reasoning network modeling framework based on ontology and natural language processing},
journal={Advanced Engineering Informatics},
year={2019},
volume={42},
doi={10.1016/j.aei.2019.100959},
art_number={100959},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068231884&doi=10.1016%2fj.aei.2019.100959&partnerID=40&md5=4e626a3b1b7d05ff874d8290add23a74},
abstract={With the advancement of scientific and engineering research, a huge number of academic literature are accumulated. Manually reviewing the existing literature is the main way to explore embedded knowledge, and the process is quite time-consuming and labor intensive. As the quantity of literature is increasing exponentially, it would be more difficult to cover all aspects of the literature using the traditional manual review approach. To overcome this drawback, bibliometric analysis is used to analyze the current situation and trend of a specific research field. In the bibliometric analysis, only a few key phrases (e.g., authors, publishers, journals, and citations) are usually used as the inputs for analysis. Information other than those phrases is not extracted for analysis, while that neglected information (e.g., abstract) might provide more detailed knowledge in the article. To tackle with this problem, this study proposed an automatic literature knowledge graph and reasoning network modeling framework based on ontology and Natural Language Processing (NLP), to facilitate the efficient knowledge exploration from literature abstract. In this framework, a representation ontology is proposed to characterize the literature abstract data into four knowledge elements (background, objectives, solutions, and findings), and NLP technology is used to extract the ontology instances from the abstract automatically. Based on the representation ontology, a four-space integrated knowledge graph is built using NLP technology. Then, reasoning network is generated according to the reasoning mechanism defined in the proposed ontology model. To validate the proposed framework, a case study is conducted to analyze the literature in the field of construction management. The case study proves that the proposed ontology model can be used to represent the knowledge embedded in the literatures’ abstracts, and the ontology elements can be automatically extracted by NLP models. The proposed framework can be an enhancement for the bibliometric analysis to explore more knowledge from the literature. © 2019 Elsevier Ltd},
author_keywords={Knowledge graph;  Knowledge reasoning;  Natural language processing;  Representation ontology},
publisher={Elsevier Ltd},
issn={14740346},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pfau201985,
author={Pfau, J. and Porzel, R. and Pomarlan, M. and Cangalovic, V.S. and Grudpan, S. and Höffner, S. and Bateman, J. and Malaka, R.},
title={Give MEANinGS to Robots with Kitchen Clash: A VR Human Computation Serious Game for World Knowledge Accumulation},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11863 LNCS},
pages={85-96},
doi={10.1007/978-3-030-34644-7_7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076997316&doi=10.1007%2f978-3-030-34644-7_7&partnerID=40&md5=2264609b92563a809a2915b5a4ccb1be},
abstract={In this paper, we introduce the framework of MEANinGS for the semi-autonomous accumulation of world knowledge for robots. Where manual aggregation is inefficient and prone to incompleteness and autonomous approaches suffer from underspecified information, we deploy the human computation game Kitchen Clash and give evidence of its efficiency, completeness and motivation potential. © IFIP International Federation for Information Processing, 2019.},
author_keywords={Framework;  Knowledge accumulation;  Serious game},
editor={van der Spek E., Gobel S., Do E., Clua E., Baalsrud Hauge J.},
publisher={Springer},
issn={03029743},
isbn={9783030346430},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Rothfuss20184007,
author={Rothfuss, J. and Ferreira, F. and Aksoy, E.E. and Zhou, Y. and Asfour, T.},
title={Deep Episodic Memory: Encoding, Recalling, and Predicting Episodic Experiences for Robot Action Execution},
journal={IEEE Robotics and Automation Letters},
year={2018},
volume={3},
number={4},
pages={4007-4014},
doi={10.1109/LRA.2018.2860057},
art_number={8421022},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062299146&doi=10.1109%2fLRA.2018.2860057&partnerID=40&md5=293c3b6259b0398b149495c34d12d187},
abstract={We present a novel deep neural network architecture for representing robot experiences in an episodic-like memory that facilitates encoding, recalling, and predicting action experiences. Our proposed unsupervised deep episodic memory model as follows: First, encodes observed actions in a latent vector space and, based on this latent encoding, second, infers most similar episodes previously experienced, third, reconstructs original episodes, and finally, predicts future frames in an end-to-end fashion. Results show that conceptually similar actions are mapped into the same region of the latent vector space. Based on these results, we introduce an action matching and retrieval mechanism, benchmark its performance on two large-scale action datasets, 20BN-something-something and ActivityNet and evaluate its generalization capability in a real-world scenario on a humanoid robot. © 2016 IEEE.},
author_keywords={deep learning in robotics and automation;  Learning and adaptive systems;  visual learning},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={23773766},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Rasheed2018467,
author={Rasheed, N. and Amin, S.H.M. and Sultana, U. and Bhatti, A.R. and Asghar, M.N.},
title={Extension of grounding mechanism for abstract words: computational methods insights},
journal={Artificial Intelligence Review},
year={2018},
volume={50},
number={3},
pages={467-494},
doi={10.1007/s10462-017-9608-9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040002206&doi=10.1007%2fs10462-017-9608-9&partnerID=40&md5=626bbaa17b2ecba43fed14c5ca81f88f},
abstract={The attempts to model cognitive phenomena effectively have split the research community in two paradigms: symbolic and connectionist. The extension of grounding phenomenon for abstract words is very important for social interactions of cognitive robots in real scenarios. This paper reviews the strength of symbolic and connectionist methods to address the abstract word grounding problem in cognitive robots. In particular, the presented work is focused on designing and simulating cognitive robotics model to achieve a grounding mechanism for abstract words by using the semantic network approach, as well as examining the utility of connectionist computation for the same problem. Two neuro-robotics models based on feed forward neural network and recurrent neural network are presented to see the pros and cons of connectionist approach. The simulation results and review of attributes of these methods reveal that the proposed symbolic model offers the solution to the problem of grounding abstract words with attributes like high data storage capacity with recall accuracy, structural integrity and temporal sequence handling. Whereas, connectionist computation based solutions give more natural solution to this problem with some shortcomings that include combinatorial ambiguity, low storage capacity and structural rigidity. The presented results are not only important for the advancement in communication system of cognitive robot, also provide evidence for embodied nature of abstract language. © 2018, Springer Science+Business Media B.V., part of Springer Nature.},
author_keywords={Abstract words;  Cognitive robotics;  Feed forward neural network;  Recurrent neural network;  Spreading activation;  Symbol grounding problem},
publisher={Springer Netherlands},
issn={02692821},
coden={AIRVE},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kunda2018155,
author={Kunda, M.},
title={Visual mental imagery: A view from artificial intelligence},
journal={Cortex},
year={2018},
volume={105},
pages={155-172},
doi={10.1016/j.cortex.2018.01.022},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049357535&doi=10.1016%2fj.cortex.2018.01.022&partnerID=40&md5=ea0266efeb1e089cbdbc0ace97e07d04},
abstract={This article investigates whether, and how, an artificial intelligence (AI) system can be said to use visual, imagery-based representations in a way that is analogous to the use of visual mental imagery by people. In particular, this article aims to answer two fundamental questions about imagery-based AI systems. First, what might visual imagery look like in an AI system, in terms of the internal representations used by the system to store and reason about knowledge? Second, what kinds of intelligent tasks would an imagery-based AI system be able to accomplish? The first question is answered by providing a working definition of what constitutes an imagery-based knowledge representation, and the second question is answered through a literature survey of imagery-based AI systems that have been developed over the past several decades of AI research, spanning task domains of: 1) template-based visual search; 2) spatial and diagrammatic reasoning; 3) geometric analogies and matrix reasoning; 4) naive physics; and 5) commonsense reasoning for question answering. This article concludes by discussing three important open research questions in the study of visual-imagery-based AI systems—on evaluating system performance, learning imagery operators, and representing abstract concepts—and their implications for understanding human visual mental imagery. © 2018 Elsevier Ltd},
author_keywords={Analogical representations;  Depictive versus descriptive;  Iconic versus propositional;  Knowledge representation;  Modal versus amodal},
publisher={Masson SpA},
issn={00109452},
coden={CRTXA},
language={English},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Ramoly2018681,
author={Ramoly, N. and Sfar, H. and Bouzeghoub, A. and Finance, B.},
title={LEAF: Using Semantic Based Experience to Prevent Task Failures},
journal={Springer Proceedings in Advanced Robotics},
year={2018},
volume={5},
pages={681-697},
doi={10.1007/978-3-319-67361-5_44},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107055484&doi=10.1007%2f978-3-319-67361-5_44&partnerID=40&md5=8ad10d92ac788a536b4faa833b70d6bd},
abstract={Using service robots at home is becoming more and more popular in order to help people in their life routine. Such robots are required to do various tasks, from user notification to devices manipulation. However, in such complex environments, robots sometimes fail to achieve one task. Failing is problematic as it is unpleasant for the user and may cause critical situations. Therefore, understanding and preventing failures is a challenging need. In this paper, we propose LEAF, an experience based approach to prevent task failure. LEAF relies on both semantic context knowledge through ontology and user validation, allowing LEAF to have an accurate understanding of failures. It then uses this new knowledge to adapt a Hierarchical Task Network (HTN) in order to prevent selecting tasks that have a high risk of failure in the plan. LEAF was tested in the Hadaptic platform and evaluated using a randomly generated dataset. © 2018, Springer International Publishing AG.},
publisher={Springer Science and Business Media B.V.},
issn={25111256},
language={English},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{Wächter2018148,
author={Wächter, M. and Ovchinnikova, E. and Wittenbeck, V. and Kaiser, P. and Szedmak, S. and Mustafa, W. and Kraft, D. and Krüger, N. and Piater, J. and Asfour, T.},
title={Integrating multi-purpose natural language understanding, robot's memory, and symbolic planning for task execution in humanoid robots},
journal={Robotics and Autonomous Systems},
year={2018},
volume={99},
pages={148-165},
doi={10.1016/j.robot.2017.10.012},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035766017&doi=10.1016%2fj.robot.2017.10.012&partnerID=40&md5=bceb9e5399f9f42bb052c737579e04b7},
abstract={We propose an approach for instructing a robot using natural language to solve complex tasks in a dynamic environment. In this study, we elaborate on a framework that allows a humanoid robot to understand natural language, derive symbolic representations of its sensorimotor experience, generate complex plans according to the current world state, and monitor plan execution. The presented development supports replacing missing objects and suggesting possible object locations. It is a realization of the concept of structural bootstrapping developed in the context of the European project Xperience. The framework is implemented within the robot development environment ArmarX. We evaluate the framework on the humanoid robot ARMAR-III in the context of two experiments: a demonstration of the real execution of a complex task in the kitchen environment on ARMAR-III and an experiment with untrained users in a simulation environment. © 2017},
author_keywords={Humanoid robotics;  Natural language understanding;  Object replacement;  Planning;  Structural bootstrapping;  Task execution},
publisher={Elsevier B.V.},
issn={09218890},
coden={RASOE},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Adjali20171,
author={Adjali, O. and Ramdane-Cherif, A.},
title={Knowledge processing using EKRL for robotic applications},
journal={International Journal of Cognitive Informatics and Natural Intelligence},
year={2017},
volume={11},
number={4},
pages={1-21},
doi={10.4018/IJCINI.2017100101},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038429830&doi=10.4018%2fIJCINI.2017100101&partnerID=40&md5=ace405a10b2407f9d76935c750d389e0},
abstract={This article describes a semantic framework that demonstrates an approach for modeling and reasoning based on environment knowledge representation language (EKRL) to enhance interaction between robots and their environment. Unlike EKRL, standard Binary approaches like OWL language fails to represent knowledge in an expressive way. The authors show in this work how to: Model environment and interaction in an expressive way with first-order and second-order EKRL data-structures, and reason for decision-making thanks to inference capabilities based on a complex unification algorithm. This is with the understanding that robot environments are inherently subject to noise and partial observability, the authors extended EKRL framework with probabilistic reasoning based on Markov logic networks to manage uncertainty. © 2017 IGI Global.},
author_keywords={Environment Knowledge Representation Language (EKRL);  Knowledge Representation;  Markov Networks;  Probabilistic Reasoning},
publisher={IGI Global},
issn={15573958},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ersen2017108,
author={Ersen, M. and Oztop, E. and Sariel, S.},
title={Cognition-Enabled Robot Manipulation in Human Environments: Requirements, Recent Work, and Open Problems},
journal={IEEE Robotics and Automation Magazine},
year={2017},
volume={24},
number={3},
pages={108-122},
doi={10.1109/MRA.2016.2616538},
art_number={7894169},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017475008&doi=10.1109%2fMRA.2016.2616538&partnerID=40&md5=1fb74f922a2b56db392bb0c8dd18b656},
abstract={Service robots are expected to play an important role in our daily lives as our companions in home and work environments in the near future. An important requirement for fulfilling this expectation is to equip robots with skills to perform everyday manipulation tasks, the success of which is crucial for most home chores, such as cooking, cleaning, and shopping. Robots have been used successfully for manipulation tasks in wellstructured and controlled factory environments for decades. Designing skills for robots working in uncontrolled human environments raises many potential challenges in various subdisciplines, such as computer vision, automated planning, and human-robot interaction. In spite of the recent progress in these fields, there are still challenges to tackle. This article outlines problems in different research areas related to mobile manipulation from the cognitive perspective, reviews recently published works and the state-of-the-art approaches to address these problems, and discusses open problems to be solved to realize robot assistants that can be used in manipulation tasks in unstructured human environments. © 1994-2011 IEEE.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={10709932},
coden={IRAME},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lemaignan201745,
author={Lemaignan, S. and Warnier, M. and Sisbot, E.A. and Clodic, A. and Alami, R.},
title={Artificial cognition for social human–robot interaction: An implementation},
journal={Artificial Intelligence},
year={2017},
volume={247},
pages={45-69},
doi={10.1016/j.artint.2016.07.002},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84998996792&doi=10.1016%2fj.artint.2016.07.002&partnerID=40&md5=0ab29fc2dafbf5db37c1e031de54ac59},
abstract={Human–Robot Interaction challenges Artificial Intelligence in many regards: dynamic, partially unknown environments that were not originally designed for robots; a broad variety of situations with rich semantics to understand and interpret; physical interactions with humans that requires fine, low-latency yet socially acceptable control strategies; natural and multi-modal communication which mandates common-sense knowledge and the representation of possibly divergent mental models. This article is an attempt to characterise these challenges and to exhibit a set of key decisional issues that need to be addressed for a cognitive robot to successfully share space and tasks with a human. We identify first the needed individual and collaborative cognitive skills: geometric reasoning and situation assessment based on perspective-taking and affordance analysis; acquisition and representation of knowledge models for multiple agents (humans and robots, with their specificities); situated, natural and multi-modal dialogue; human-aware task planning; human–robot joint task achievement. The article discusses each of these abilities, presents working implementations, and shows how they combine in a coherent and original deliberative architecture for human–robot interaction. Supported by experimental results, we eventually show how explicit knowledge management, both symbolic and geometric, proves to be instrumental to richer and more natural human–robot interactions by pushing for pervasive, human-level semantics within the robot's deliberative system. © 2017 The Authors},
author_keywords={Cognitive architecture;  Cognitive robotics;  Human–robot interaction;  Knowledge representation and reasoning;  Perspective taking},
publisher={Elsevier B.V.},
issn={00043702},
coden={AINTB},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kunze2017352,
author={Kunze, L. and Beetz, M.},
title={Envisioning the qualitative effects of robot manipulation actions using simulation-based projections},
journal={Artificial Intelligence},
year={2017},
volume={247},
pages={352-380},
doi={10.1016/j.artint.2014.12.004},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921450087&doi=10.1016%2fj.artint.2014.12.004&partnerID=40&md5=7e2b100acc6d29c12a584d38e1efb2cb},
abstract={Autonomous robots that are to perform complex everyday tasks such as making pancakes have to understand how the effects of an action depend on the way the action is executed. Within Artificial Intelligence, classical planning reasons about whether actions are executable, but makes the assumption that the actions will succeed (with some probability). In this work, we have designed, implemented, and analyzed a framework that allows us to envision the physical effects of robot manipulation actions. We consider envisioning to be a qualitative reasoning method that reasons about actions and their effects based on simulation-based projections. Thereby it allows a robot to infer what could happen when it performs a task in a certain way. This is achieved by translating a qualitative physics problem into a parameterized simulation problem; performing a detailed physics-based simulation of a robot plan; logging the state evolution into appropriate data structures; and then translating these sub-symbolic data structures into interval-based first-order symbolic, qualitative representations, called timelines. The result of the envisioning is a set of detailed narratives represented by timelines which are then used to infer answers to qualitative reasoning problems. By envisioning the outcome of actions before committing to them, a robot is able to reason about physical phenomena and can therefore prevent itself from ending up in unwanted situations. Using this approach, robots can perform manipulation tasks more efficiently, robustly, and flexibly, and they can even successfully accomplish previously unknown variations of tasks. © 2014 Elsevier B.V.},
author_keywords={Envisioning;  Everyday robot manipulation;  Naive physics;  Simulation-based projections},
publisher={Elsevier B.V.},
issn={00043702},
coden={AINTB},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pratama2017313,
author={Pratama, F. and Mastrogiovanni, F. and Lee, S.G. and Chong, N.Y.},
title={Long-term knowledge acquisition using contextual information in a memory-inspired robot architecture},
journal={Journal of Experimental and Theoretical Artificial Intelligence},
year={2017},
volume={29},
number={2},
pages={313-334},
doi={10.1080/0952813X.2015.1134679},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958038865&doi=10.1080%2f0952813X.2015.1134679&partnerID=40&md5=7b3c8e71ca137ac2611f4c850a4327c7},
abstract={In this paper, we present a novel cognitive framework allowing a robot to form memories of relevant traits of its perceptions and to recall them when necessary. The framework is based on two main principles: on the one hand, we propose an architecture inspired by current knowledge in human memory organisation; on the other hand, we integrate such an architecture with the notion of context, which is used to modulate the knowledge acquisition process when consolidating memories and forming new ones, as well as with the notion of familiarity, which is employed to retrieve proper memories given relevant cues. Although much research has been carried out, which exploits Machine Learning approaches to provide robots with internal models of their environment (including objects and occurring events therein), we argue that such approaches may not be the right direction to follow if a long-term, continuous knowledge acquisition is to be achieved. As a case study scenario, we focus on both robot–environment and human–robot interaction processes. In case of robot–environment interaction, a robot performs pick and place movements using the objects in the workspace, at the same time observing their displacement on a table in front of it, and progressively forms memories defined as relevant cues (e.g. colour, shape or relative position) in a context-aware fashion. As far as human–robot interaction is concerned, the robot can recall specific snapshots representing past events using both sensory information and contextual cues upon request by humans. © 2016 Informa UK Limited, trading as Taylor & Francis Group.},
author_keywords={context-based memory retrieval;  developmental learning;  long-term knowledge acquisition;  Robot cognitive architectures},
publisher={Taylor and Francis Ltd.},
issn={0952813X},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Valipour201797,
author={Valipour, M. and Wang, Y. and Zatarain, O.A. and Gavrilova, M.L.},
title={Algorithms for determining semantic relations of formal concepts by cognitive machine learning based on concept algebra},
journal={Proceedings of 2016 IEEE 15th International Conference on Cognitive Informatics and Cognitive Computing, ICCI*CC 2016},
year={2017},
pages={97-105},
doi={10.1109/ICCI-CC.2016.7862021},
art_number={7862021},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016167034&doi=10.1109%2fICCI-CC.2016.7862021&partnerID=40&md5=c63a36da2da3f4b38d216be5a47534a0},
abstract={It is recognized that the semantic space of knowledge is a hierarchical concept network. This paper presents theories and algorithms of hierarchical concept classification by quantitative semantic relations via machine learning based on concept algebra. The equivalence between formal concepts are analyzed by an Algorithm of Concept Equivalence Analysis (ACEA), which quantitatively determines the semantic similarity of an arbitrary pair of formal concepts. This leads to the development of the Algorithm of Relational Semantic Classification (ARSC) for hierarchically classify any given concept in the semantic space of knowledge. Experiments applying Algorithms ACEA and ARSC on 20 formal concepts are successfully conducted, which encouragingly demonstrate the deep machine understanding of semantic relations and their quantitative weights beyond human perspectives on knowledge learning and natural language processing. © 2016 IEEE.},
author_keywords={Cognitive algorithms;  Cognitive machine learning;  Concept algebra;  Concept classification;  Concept hierarchy;  Concept similarity;  Formal concept;  Knowledge representation;  Semantic analysis},
editor={Plataniotis K., Widrow B., Howard N., Zadeh L.A., Wang Y.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781509038466},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Al-Hunaiyyan201775,
author={Al-Hunaiyyan, A. and Bimba, A.T. and Idris, N. and Al-Sharhan, S.},
title={A cognitive knowledge-based framework for social and metacognitive support in mobile learning},
journal={Interdisciplinary Journal of Information, Knowledge, and Management},
year={2017},
volume={12},
pages={75-98},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016085381&partnerID=40&md5=755f790c0e97f2a1c5ceb054dd3b6016},
abstract={Aim/Purpose: This work aims to present a knowledge modeling technique that supports the representation of the student learning process and that is capable of providing a means for self-assessment and evaluating newly acquired knowledge. The objective is to propose a means to address the pedagogical challenges in mlearning by aiding students' metacognition through a model of a student with the target domain and pedagogy. Background: This research proposes a framework for social and meta-cognitive support to tackle the challenges raised. Two algorithms are introduced: the meta-cognition algorithm for representing the student's learning process, which is capable of providing a means for self-assessment, and the social group mapping algorithm for classifying students according to social groups. Methodology: Based on the characteristics of knowledge in an m-learning system, the cognitive knowledge base is proposed for knowledge elicitation and representation. The proposed technique allows a proper categorization of students to support collaborative learning in a social platform by utilizing the strength of m-learning in a social context. The social group mapping and metacognition algorithms are presented. Contribution: The proposed model is envisaged to serve as a guide for developers in implementing suitable m-learning applications. Furthermore, educationists and instructors can devise new pedagogical practices based on the possibilities provided by the proposed m-learning framework. Findings: The effectiveness of any knowledge management system is grounded in the technique used in representing the knowledge. The CKB proposed manipulates knowledge as a dynamic concept network, similar to human knowledge processing, thus, providing a rich semantic capability, which provides various relationships between concepts. Recommendations for Practitioners Recommendation for Researchers: Educationist and instructors need to develop new pedagogical practices in line with m-learning. The design and implementation of an effective m-learning application are chal lenging due to the reliance on both pedagogical and technological elements. To tackle this challenge, frameworks which describe the conceptual interaction between the various components of pedagogy and technology need to be proposed. Impact on Society: The creation of an educational platform that provides instant access to relevant knowledge. Future Research: In the future, the proposed framework will be evaluated against some set of criteria for its effectiveness in acquiring and presenting knowledge in a real-life scenario. By analyzing real student interaction in m-learning, the algorithms will be tested to show their applicability in eliciting student metacognition and support for social interactivity.},
author_keywords={Knowledge modeling;  M-learning;  Mobile device;  Pedagogy},
publisher={Informing Science Institute},
issn={15551229},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bimba2016857,
author={Bimba, A.T. and Idris, N. and Al-Hunaiyyan, A. and Mahmud, R.B. and Abdelaziz, A. and Khan, S. and Chang, V.},
title={Towards knowledge modeling and manipulation technologies: A survey},
journal={International Journal of Information Management},
year={2016},
volume={36},
number={6},
pages={857-871},
doi={10.1016/j.ijinfomgt.2016.05.022},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975106405&doi=10.1016%2fj.ijinfomgt.2016.05.022&partnerID=40&md5=7c4a86cde3ec9f5131db7e6c9faf4292},
abstract={A system which represents knowledge is normally referred to as a knowledge based system (KBS). This article focuses on surveying publications related to knowledge base modelling and manipulation technologies, between the years 2000-2015. A total of 185 articles excluding the subject descriptive articles which are mentioned in the introductory parts, were evaluated in this survey. The main aim of this study is to identify different knowledge base modelling and manipulation techniques based on 4 categories; 1) linguistic knowledge base; 2) expert knowledge base; 3) ontology and 4) cognitive knowledge base. This led to the proposition of 8 research questions, which focused on the different categories of knowledge base modelling technologies, their underlying theories, knowledge representation technique, knowledge acquisition technique, challenges, applications, development tools and development languages. A part of the findings from this survey is the high dependence of linguistic knowledge base, expert knowledge base and ontology on volatile expert knowledge. A promising technique for knowledge-based business management and other knowledge related applications is also discussed. © 2016 Elsevier Ltd. All rights reserved.},
author_keywords={Cognitive knowledge base;  Expert knowledge base;  Knowledge acquisition;  Knowledge-based business;  Linguistic knowledge base;  Ontology},
publisher={Elsevier Ltd},
issn={02684012},
coden={IJMAE},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Liu20161,
author={Liu, R. and Zhang, X.},
title={Fuzzy context-specific intention inference for robotic caregiving},
journal={International Journal of Advanced Robotic Systems},
year={2016},
volume={13},
number={5},
pages={1-14},
doi={10.1177/1729881416662780},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993940017&doi=10.1177%2f1729881416662780&partnerID=40&md5=45f4a072531cce2723e26d73c64d2fd2},
abstract={To provide timely and appropriate assistance, robots must have the capability of proactively understanding a user's personal needs, the so-called human intention inference. In human-human interaction, humans have a natural and implicit way to infer others' intentions by selecting correlated context features and interpreting these features based on their life experience. However, robots do not have this capability and it is not realistic to build an explicit formula to associate human intentions with context. In this article, a novel fuzzy context-specific intention inference method is developed for human-like implicit human intention inference. With a fuzzy manner, context features are converted into discrete context statuses, which are similar to human subjective feelings. An intention-centered common sense database is developed consisting of correlated fuzzy context statuses, object affordances, and their relationship with human intentions. With this database, a Fuzzy Naïve Bayesian Network algorithm is adopted for implicit intention inference. Home scenario results validated the fuzzy context-specific intention inference methods reliability and lab scenario results validated the fuzzy context-specific intention inference methods effectiveness and robustness. This work is expected to develop intuitive and effective human-robot interaction, consequently enhancing the adoption of assistive technologies and improving the independence of the disabled and elderly in activities of daily living. © SAGE Publications Ltd, unless otherwise noted. Manuscript content on this site is licensed under Creative Commons Licenses.},
author_keywords={context-intention correlation;  fuzzy context;  fuzzy Naïve Bayesian;  Human-robot interaction;  intention inference},
publisher={SAGE Publications Inc.},
issn={17298806},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang201613,
author={Wang, Y. and Valipour, M. and Zatarain, O.A.},
title={Quantitative semantic analysis and comprehension by cognitive machine learning},
journal={International Journal of Cognitive Informatics and Natural Intelligence},
year={2016},
volume={10},
number={3},
pages={13-28},
doi={10.4018/IJCINI.2016070102},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84998631503&doi=10.4018%2fIJCINI.2016070102&partnerID=40&md5=23902cdceb1c0da1be731c121dd1cfb5},
abstract={Knowledge learning is the sixth and the most fundamental category of machine learning mimicking the brain. It is recognized that the semantic space of machine knowledge is a hierarchical concept network (HCN), which can be rigorously represented by formal concepts in concept algebra and semantic algebra. This paper presents theories and algorithms of hierarchical concept classification by quantitative semantic analysis based on machine learning. Semantic equivalence between formal concepts is rigorously measured by an Algorithm of Concept Equivalence Analysis (ACEA). The semantic hierarchy among formal concepts is quantitatively determined by an Algorithm of Relational Semantic Classification (ARSC). Experiments applying Algorithms ACEA and ARSC on a set of formal concepts have been successfully conducted, which demonstrate a deep machine understanding of formal concepts and quantitative relations in the hierarchical semantic space by machine learning beyond human empirical perspectives. Copyright © 2016, IGI Global.},
author_keywords={Algorithms;  Cognitive Learning;  Concept Algebra;  Concept Classification;  Knowledge Learning;  Knowledge Representation;  Machine Learning;  Semantic Algebra;  Semantic Analysis},
publisher={IGI Global},
issn={15573958},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Erdogan2016471,
author={Erdogan, C. and Stilman, M.},
title={Autonomous realization of simple machines},
journal={Springer Tracts in Advanced Robotics},
year={2016},
volume={109},
pages={471-486},
doi={10.1007/978-3-319-23778-7_31},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948176390&doi=10.1007%2f978-3-319-23778-7_31&partnerID=40&md5=92dc2ccbfc50e2f2bc17a9ab32d0dfa4},
abstract={For robots to become integral parts of human daily experience, they need to be able to utilize the objects in their environment to accomplish any range of tasks. In this work, we focus particularly on physically challenging tasks that push the limits on the robot kinodynamic constraints such as joint limits, joint torques and etc. Previously, we demonstrated an autonomous planner that instructs a human collaborator where to place the available objects in the environment to form a simple machine such as a lever-fulcrum assembly. In this work, we report results on the autonomous realization of such a design by the humanoid robot Golem Krang, focusing on the challenges of autonomous perception, manipulation and control. © Springer International Publishing Switzerland 2016.},
publisher={Springer Verlag},
issn={16107438},
language={English},
document_type={Book Chapter},
source={Scopus},
}

@CONFERENCE{Pratama201525,
author={Pratama, F. and Mastrogiovanni, F. and Jeong, S. and Chong, N.Y.},
title={Long-term knowledge acquisition in a memory-based epigenetic robot architecture for verbal interaction},
journal={Proceedings - IEEE International Workshop on Robot and Human Interactive Communication},
year={2015},
volume={2015-November},
pages={25-30},
doi={10.1109/ROMAN.2015.7333563},
art_number={7333563},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954073823&doi=10.1109%2fROMAN.2015.7333563&partnerID=40&md5=2bea7912ecb4065266a1fa91f13a2041},
abstract={We present a robot cognitive framework based on (a) a memory-like architecture; and (b) the notion of 'context'. We posit that relying solely on machine learning techniques may not be the right approach for a long-term, continuous knowledge acquisition. Since we are interested in long-term human-robot interaction, we focus on a scenario where a robot 'remembers' relevant events happening in the environment. By visually sensing its surroundings, the robot is expected to infer and remember snapshots of events, and recall specific past events based on inputs and contextual information from humans. Using a COTS vision frameworks for the experiment, we show that the robot is able to form 'memories' and recall related events based on cues and the context given during the human-robot interaction process. © 2015 IEEE.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781467367042},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Beetz20151983,
author={Beetz, M. and Tenorth, M. and Winkler, J.},
title={Open-EASE},
journal={Proceedings - IEEE International Conference on Robotics and Automation},
year={2015},
volume={2015-June},
number={June},
pages={1983-1990},
doi={10.1109/ICRA.2015.7139458},
art_number={7139458},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938264513&doi=10.1109%2fICRA.2015.7139458&partnerID=40&md5=c7e147aea7514f2a1a0b6bae849cd0d2},
abstract={Making future autonomous robots capable of accomplishing human-scale manipulation tasks requires us to equip them with knowledge and reasoning mechanisms. We propose Open-EASE, a remote knowledge representation and processing service that aims at facilitating these capabilities. Open-EASE gives its users unprecedented access to the knowledge of leading-edge autonomous robotic agents. It also provides the representational infrastructure to make inhomogeneous experience data from robots and human manipulation episodes semantically accessible, and is complemented by a suite of software tools that enable researchers and robots to interpret, analyze, visualize, and learn from the experience data. Using Open-EASE users can retrieve the memorized experiences of manipulation episodes and ask queries regarding to what the robot saw, reasoned, and did as well as how the robot did it, why, and what effects it caused. © 2015 IEEE.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={10504729},
isbn={9781479969234},
coden={PIIAE},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Riazuelo2015432,
author={Riazuelo, L. and Tenorth, M. and Di Marco, D. and Salas, M. and Gálvez-López, D. and Mösenlechner, L. and Kunze, L. and Beetz, M. and Tardós, J.D. and Montano, L. and Montiel, J.M.M.},
title={RoboEarth Semantic Mapping: A Cloud Enabled Knowledge-Based Approach},
journal={IEEE Transactions on Automation Science and Engineering},
year={2015},
volume={12},
number={2},
pages={432-443},
doi={10.1109/TASE.2014.2377791},
art_number={7015601},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027971073&doi=10.1109%2fTASE.2014.2377791&partnerID=40&md5=df9f81c018619a848028f3a1015feeca},
abstract={The vision of the RoboEarth project is to design a knowledge-based system to provide web and cloud services that can transform a simple robot into an intelligent one. In this work, we describe the RoboEarth semantic mapping system. The semantic map is composed of: 1) an ontology to code the concepts and relations in maps and objects and 2) a SLAM map providing the scene geometry and the object locations with respect to the robot. We propose to ground the terminological knowledge in the robot perceptions by means of the SLAM map of objects. RoboEarth boosts mapping by providing: 1) a subdatabase of object models relevant for the task at hand, obtained by semantic reasoning, which improves recognition by reducing computation and the false positive rate; 2) the sharing of semantic maps between robots; and 3) software as a service to externalize in the cloud the more intensive mapping computations, while meeting the mandatory hard real time constraints of the robot. To demonstrate the RoboEarth cloud mapping system, we investigate two action recipes that embody semantic map building in a simple mobile robot. The first recipe enables semantic map building for a novel environment while exploiting available prior information about the environment. The second recipe searches for a novel object, with the efficiency boosted thanks to the reasoning on a semantically annotated map. Our experimental results demonstrate that, by using RoboEarth cloud services, a simple robot can reliably and efficiently build the semantic maps needed to perform its quotidian tasks. In addition, we show the synergetic relation of the SLAM map of objects that grounds the terminological knowledge coded in the ontology. © 2004-2012 IEEE.},
author_keywords={Cloud mapping;  knowledge representation;  object recognition;  semantic mapping;  visual SLAM},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15455955},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Pratama201468,
author={Pratama, F. and Mastrogiovanni, F. and Chong, N.Y.},
title={An integrated epigenetic robot architecture via context-influenced long-term memory},
journal={IEEE ICDL-EPIROB 2014 - 4th Joint IEEE International Conference on Development and Learning and on Epigenetic Robotics},
year={2014},
pages={68-74},
doi={10.1109/DEVLRN.2014.6982956},
art_number={6982956},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920926757&doi=10.1109%2fDEVLRN.2014.6982956&partnerID=40&md5=1bd936f5c43d62271871a663a11d0b63},
abstract={In this paper, we present a conceptual design for a context-influenced Long-Term Memory architecture. The notion of context is used as a means to organize the information flow between the Working Memory and Long-Term Memory components. In particular, we discuss the major influence of the notion of context within the Episodic Memory on the Semantic and Procedural Memory, respectively. In other words, we address how the occurrence of specific events in time impacts on the meaning of those events and the subsequent use of objects through robot actions. The general architecture design and its implementation in a simulated scenario are described. Such issues as memory items representation, individual structures of Long-Term Memory components, as well as memory-based recognition and item retrieval processes, are discussed in detail. © 2014 IEEE.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781479975402},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bossemeyer2014759,
author={Bossemeyer, M.T. and Aydin, K.T. and Metzler, T. and Lindemann, U.},
title={Analysis of the systematic approach to develop cognitive products in academic research projects},
journal={Proceedings of International Design Conference, DESIGN},
year={2014},
volume={2014-January},
pages={759-770},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958055906&partnerID=40&md5=1580fb25b896e9619eeed1d8260bf530},
author_keywords={Cognitive products;  Development methods;  Procedural model;  Systematic development},
editor={Marjanovic D., Storga M., Bojcetic N., Pavkovic N.},
publisher={Faculty of Mechanical Engineering and Naval Architecture},
issn={18479073},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wörgötter2013117,
author={Wörgötter, F. and Aksoy, E.E. and Krüger, N. and Piater, J. and Ude, A. and Tamosiunaite, M.},
title={A simple ontology of manipulation actions based on hand-object relations},
journal={IEEE Transactions on Autonomous Mental Development},
year={2013},
volume={5},
number={2},
pages={117-134},
doi={10.1109/TAMD.2012.2232291},
art_number={6493410},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879400409&doi=10.1109%2fTAMD.2012.2232291&partnerID=40&md5=0a5e71ffc82fdd80d6fb2ded75120ad0},
abstract={Humans can perform a multitude of different actions with their hands (manipulations). In spite of this, so far there have been only a few attempts to represent manipulation types trying to understand the underlying principles. Here we first discuss how manipulation actions are structured in space and time. For this we use as temporal anchor points those moments where two objects (or hand and object) touch or un-touch each other during a manipulation. We show that by this one can define a relatively small tree-like manipulation ontology. We find less than 30 fundamental manipulations. The temporal anchors also provide us with information about when to pay attention to additional important information, for example when to consider trajectory shapes and relative poses between objects. As a consequence a highly condensed representation emerges by which different manipulations can be recognized and encoded. Examples of manipulations recognition and execution by a robot based on this representation are given at the end of this study. © 2013 IEEE.},
author_keywords={Manipulation action;  manipulation ontology;  scene graph;  semantic event chain},
issn={19430604},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Daoutis2013179,
author={Daoutis, M.},
title={Knowledge Based Perceptual Anchoring: Grounding Percepts to Concepts in Cognitive Robots},
journal={KI - Kunstliche Intelligenz},
year={2013},
volume={27},
number={2},
pages={179-182},
doi={10.1007/s13218-013-0249-0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087964811&doi=10.1007%2fs13218-013-0249-0&partnerID=40&md5=6919694a2674ca32a856bb71555f54ff},
abstract={Perceptual anchoring is the process of creating and maintaining a connection between the sensor data corresponding to a physical object and its symbolic description. It is a subset of the symbol grounding problem, introduced by Harnad (Phys. D, Nonlinear Phenom. 42(1–3):335–346, 1990) and investigated over the past years in several disciplines including robotics. This PhD dissertation focuses on a method for grounding sensor data of physical objects to the corresponding semantic descriptions, in the context of cognitive robots where the challenge is to establish the connection between percepts and concepts referring to objects, their relations and properties. We examine how knowledge representation can be used together with an anchoring framework, so as to complement the meaning of percepts while supporting better linguistic interaction with the use of the corresponding concepts. The proposed method addresses the need to represent and process both perceptual and semantic knowledge, often expressed in different abstraction levels, while originating from different modalities. We then focus on the integration of anchoring with a large scale knowledge base system and with perceptual routines. This integration is applied in a number of studies, where in the context of a smart home, several evaluations spanning from spatial and commonsense reasoning to linguistic interaction and concept acquisition. © 2013, Springer-Verlag Berlin Heidelberg.},
author_keywords={Anchoring;  Cognitive perception;  Common-sense information;  Knowledge representation;  Symbol grounding},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={09331875},
language={English},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tenorth2013566,
author={Tenorth, M. and Beetz, M.},
title={KnowRob: A knowledge processing infrastructure for cognition-enabled robots},
journal={International Journal of Robotics Research},
year={2013},
volume={32},
number={5},
pages={566-590},
doi={10.1177/0278364913481635},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878084306&doi=10.1177%2f0278364913481635&partnerID=40&md5=c9c323b20ad5c7d497f6a450b13ed5ab},
abstract={Autonomous service robots will have to understand vaguely described tasks, such as "set the table" or "clean up". Performing such tasks as intended requires robots to fully, precisely, and appropriately parameterize their low-level control programs. We propose knowledge processing as a computational resource for enabling robots to bridge the gap between vague task descriptions and the detailed information needed to actually perform those tasks in the intended way. In this article, we introduce the KnowRob knowledge processing system that is specifically designed to provide autonomous robots with the knowledge needed for performing everyday manipulation tasks. The system allows the realization of "virtual knowledge bases": collections of knowledge pieces that are not explicitly represented but computed on demand from the robot's internal data structures, its perception system, or external sources of information. This article gives an overview of the different kinds of knowledge, the different inference mechanisms, and interfaces for acquiring knowledge from external sources, such as the robot's perception system, observations of human activities, Web sites on the Internet, as well as Web-based knowledge bases for information exchange between robots. We evaluate the system's scalability and present different integrated experiments that show its versatility and comprehensiveness. © The Author(s) 2013.},
author_keywords={grounded reasoning methods;  knowledge bases for robots;  knowledge representation},
issn={02783649},
coden={IJRRE},
language={English},
document_type={Article},
source={Scopus},
}

@BOOK{Ligozat2013,
author={Ligozat, G.},
title={Qualitative Spatial and Temporal Reasoning},
journal={Qualitative Spatial and Temporal Reasoning},
year={2013},
page_count={505},
doi={10.1002/9781118601457},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867203839&doi=10.1002%2f9781118601457&partnerID=40&md5=5571c9d5739627485f6d85c8e0e35a99},
abstract={Starting with an updated description of Allen's calculus, the book proceeds with a description of the main qualitative calculi which have been developed over the last two decades. It describes the connection of complexity issues to geometric properties. Models of the formalisms are described using the algebraic notion of weak representations of the associated algebras. The book also includes a presentation of fuzzy extensions of qualitative calculi, and a description of the study of complexity in terms of clones of operations. © ISTE Ltd 2012.},
publisher={John Wiley and Sons},
isbn={9781848212527},
language={English},
document_type={Book},
source={Scopus},
}

@CONFERENCE{Al-Moadhen2013211,
author={Al-Moadhen, A. and Qiu, R. and Packianather, M. and Ji, Z. and Setchi, R.},
title={Integrating robot task planner with common-sense knowledge base to improve the efficiency of planning},
journal={Procedia Computer Science},
year={2013},
volume={22},
pages={211-220},
doi={10.1016/j.procs.2013.09.097},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896971021&doi=10.1016%2fj.procs.2013.09.097&partnerID=40&md5=18a57a3f092386d32e83831cb76beca6},
abstract={This paper presents a developed approach for intelligently generating symbolic plans by mobile robots acting in domestic environments, such as offices and houses. The significance of the approach lies in developing a new framework that consists of the new modeling of high-level robot actions and then their integration with common-sense knowledge in order to support a robotic task planner. This framework will enable interactions between the task planner and the semantic knowledge base directly. By using common-sense domain knowledge, the task planner will take into consideration the properties and relations of objects and places in its environment, before creating semantically related actions that will represent a plan. This plan will accomplish the user order. The robot task planner will use the available domain knowledge to check the next related actions to the current one and the action's conditions met will be chosen. Then the robot will use the immediately available knowledge information to check whether the plan outcomes are met or violated. © 2013 The Authors.},
author_keywords={Common-Sense knowledge;  ConceptNet;  Semantic Action Model;  Semantic Network;  Task Planner},
publisher={Elsevier B.V.},
issn={18770509},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Daoutis2012213,
author={Daoutis, M. and Coradeschi, S. and Loutfi, A.},
title={Towards concept anchoring for cognitive robots},
journal={Intelligent Service Robotics},
year={2012},
volume={5},
number={4},
pages={213-228},
doi={10.1007/s11370-012-0117-z},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867580722&doi=10.1007%2fs11370-012-0117-z&partnerID=40&md5=db737ce324bc75b3c394e03c1ee262ca},
abstract={We present a model for anchoring categorical conceptual information which originates from physical perception and the web. The model is an extension of the anchoring framework which is used to create and maintain over time semantically grounded sensor information. Using the augmented anchoring framework that employs complex symbolic knowledge from a commonsense knowledge base, we attempt to ground and integrate symbolic and perceptual data that are available on the web. We introduce conceptual anchors which are representations of general, concrete conceptual terms. We show in an example scenario how conceptual anchors can be coherently integrated with perceptual anchors and commonsense information for the acquisition of novel concepts. © 2012 Springer-Verlag.},
author_keywords={Anchoring;  Categorical perception;  Commonsense information;  Knowledge representation;  Near sets},
publisher={Springer Verlag},
issn={18612776},
language={English},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Tenorth201281,
author={Tenorth, M. and Beetz, M.},
title={Knowledge processing for autonomous robot control},
journal={AAAI Spring Symposium - Technical Report},
year={2012},
volume={SS-12-02},
pages={81-86},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864870108&partnerID=40&md5=878895e60aba4048e6015cdc52960187},
abstract={Successfully accomplishing everyday manipulation tasks requires robots to have substantial knowledge about the objects they interact with, the environment they operate in as well as about the properties and effects of the actions they perform. Often, this knowledge is implicitly contained in manually written control programs, which makes it hard for the robot to adapt to newly acquired information or to re-use knowledge in a different context. By explicitly representing this knowledge, control decisions can be formulated as inference tasks which can be sent as queries to a knowledge base. This allows the robot to take all information it has at query time into account to generate answers, leading to better flexibility, adaptability to changing situations, robustness, and the ability to re-use knowledge once acquired. In this paper, we report on our work towards a practical and grounded knowledge representation and inference system. The system is specifically designed to meet the challenges created by using knowledge processing techniques on autonomous robots, including specialized inference methods, grounding of symbolic knowledge in the robot's control structures, and the acquisition of the different kinds of knowledge a robot needs. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.},
isbn={9781577355519},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@BOOK{Proudfoot2012,
author={Proudfoot, D. and Copeland, B.J.},
title={Artificial Intelligence},
journal={The Oxford Handbook of Philosophy of Cognitive Science},
year={2012},
doi={10.1093/oxfordhb/9780195309799.013.0007},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923515763&doi=10.1093%2foxfordhb%2f9780195309799.013.0007&partnerID=40&md5=5f995aa401441ee4235d3e95a26eecec},
abstract={In this article the central philosophical issues concerning human-level artificial intelligence (AI) are presented. AI largely changed direction in the 1980s and 1990s, concentrating on building domain-specific systems and on subgoals such as self-organization, self-repair, and reliability. Computer scientists aimed to construct intelligence amplifiers for human beings, rather than imitation humans. Turing based his test on a computer-imitates-human game, describing three versions of this game in 1948, 1950, and 1952. The famous version appears in a 1950 article in Mind, 'Computing Machinery and Intelligence' (Turing 1950). The interpretation of Turing's test is that it provides an operational definition of intelligence (or thinking) in machines, in terms of behavior. 'Intelligent Machinery' sets out the thesis that whether an entity is intelligent is determined in part by our responses to the entity's behavior. Wittgenstein frequently employed the idea of a human being acting like a reliable machine. A 'living reading-machine' is a human being or other creature that is given written signs, for example Chinese characters, arithmetical symbols, logical symbols, or musical notation, and who produces text spoken aloud, solutions to arithmetical problems, and proofs of logical theorems. Wittgenstein mentions that an entity that manipulates symbols genuinely reads only if he or she has a particular history, involving learning and training, and participates in a social environment that includes normative constraints and further uses of the symbols. © 2012 Oxford University Press. All rights reserved.},
author_keywords={Artificial intelligence;  Chinese room argument;  Imitation game;  Kurzweil's law;  Turing test},
publisher={Oxford University Press},
isbn={9780199940967; 9780195309799},
language={English},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{Wood201281,
author={Wood, R. and Baxter, P. and Belpaeme, T.},
title={A review of long-term memory in natural and synthetic systems},
journal={Adaptive Behavior},
year={2012},
volume={20},
number={2},
pages={81-103},
doi={10.1177/1059712311421219},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857954607&doi=10.1177%2f1059712311421219&partnerID=40&md5=e2cbf8b2074db1765322859d8110199c},
abstract={Memory may be broadly regarded as information gained from past experience that is available in the service of ongoing and future adaptive behavior. The biological implementation of memory shares little with memory in synthetic cognitive systems where it is typically regarded as a passive storage structure. Neurophysiological evidence indicates that memory is neither passive nor centralized. A review of the relevant literature in the biological and computer sciences is conducted and a novel methodology is applied that incorporates neuroethological approaches with general biological inspiration in the design of synthetic cognitive systems: a case study regarding episodic memory provides an illustration of the utility of this methodology. As a consequence of applying this approach to the reinterpretation of the implementation of memory in synthetic systems, four fundamental functional principles are derived that are in accordance with neuroscientific theory, and which may be applied to the design of more adaptive and robust synthetic cognitive systems: priming, cross-modal associations, cross-modal coordination without semantic information transfer, and global system behavior resulting from activation dynamics within the memory system. © The Author(s) 2011.},
author_keywords={cognitive systems;  Memory;  neuroethology;  robotics},
issn={10597123},
language={English},
document_type={Article},
source={Scopus},
}
