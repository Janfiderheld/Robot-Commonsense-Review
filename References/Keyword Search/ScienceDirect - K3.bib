@article{JI20142071,
title = {A weighted causal theory for acquiring and utilizing open knowledge},
journal = {International Journal of Approximate Reasoning},
volume = {55},
number = {9},
pages = {2071-2082},
year = {2014},
note = {Weighted Logics for Artificial Intelligence},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2014.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X14000486},
author = {Jianmin Ji and Xiaoping Chen},
keywords = {Nonmonotonic causal theories, Service robots, Abductive reasoning},
abstract = {Motivated by enabling intelligent robots/agents to take advantage of open-source knowledge resources to solve open-ended tasks, a weighted causal theory is introduced as the formal basis for the development of these robots/agents. The action model of a robot/agent is specified as a causal theory following McCain and Turner's nonmonotonic causal theories. New knowledge is needed when the robot/agent is given a user task that cannot be accomplished only with the action model. This problem is cast as a variant of abduction, that is, to find the most suitable set of causal rules from open-source knowledge resources, so that a plan for accomplishing the task can be computed using the action model together with the acquired knowledge. The core part of our theory is constructed based on credulous reasoning and the complexity of corresponding abductive reasoning is analyzed. The entire theory is established by adding weights to hypothetical causal rules and using them to compare competing explanations which induce causal models satisfying the task. Moreover, we sketch a model theoretic semantics for the weighted causal theory and present an algorithm for computing a weighted-abductive explanation. An application of the techniques proposed in this paper is illustrated in an example on our service robot, KeJia, in which the robot tries to acquire proper knowledge from OMICS, a large-scale open-source knowledge resource, and solve new tasks with the knowledge.}
}
@article{NG2021103298,
title = {Weakly supervised action segmentation with effective use of attention and self-attention},
journal = {Computer Vision and Image Understanding},
volume = {213},
pages = {103298},
year = {2021},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2021.103298},
url = {https://www.sciencedirect.com/science/article/pii/S1077314221001429},
author = {Yan Bin Ng and Basura Fernando},
keywords = {Weakly supervised action segmentation, Self-attention, Sequence-to-sequence models},
abstract = {This paper generates human action sequences using a novel hybrid sequence-to-sequence model that outputs a sequence of actions in the chronological order of the actions being performed in the longer activity of a given video. At test time, our models are able to generate action for each frame using weak supervision. We evaluate several sequence-to-sequence models to solve this task and demonstrate that they are able to solve action segment generation tasks on three challenging action recognition datasets. We present how to use self-attention and standard attention mechanisms with known sequence-to-sequence models for weakly supervised video action segmentation. Our new architecture is effective for weakly supervised action segmentation that uses a combination of recurrent and transformer-based sequence-to-sequence models. Our architecture consists of Transformers and GRU encoders to encode temporal information and we use self-attention and standard attention during the decoding process. We introduce an effective positional weight prior to further improve action segmentation performance. Using this architecture and two types of attention along with positional weight priors, we obtain state-of-the-art results on Breakfast and 50Salads datasets for weakly supervised action segmentation.}
}
@article{FENG2019121,
title = {Story co-segmentation of Chinese broadcast news using weakly-supervised semantic similarity},
journal = {Neurocomputing},
volume = {355},
pages = {121-133},
year = {2019},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219306629},
author = {Wei Feng and Xuecheng Nie and Yujun Zhang and Zhi-Qiang Liu and Jianwu Dang},
keywords = {Story co-segmentation, Weakly-supervised correlated affinity graph (WSCAG), Parallel affinity propagation, Generalized cosine similarity, Chinese broadcast news, MRF},
abstract = {This paper presents lexical story co-segmentation, a new approach to automatically extracting stories on the same topic from multiple Chinese broadcast news documents. Unlike topic tracking and detection, our approach needs not the guidance of well-trained topic models and can consistently segment the common stories from input documents. Following the MRF scheme, we construct a Gibbs energy function that feasibly balances the intra-doc and inter-doc lexical semantic dependencies and solve story co-segmentation as a binary labeling problem at sentence level. Due to the significance of measuring lexical semantic similarity in story co-segmentation, we propose a weakly-supervised correlated affinity graph (WSCAG) model to effectively derive the latent semantic similarities between Chinese words from the target corpus. Based on this, we are able to extend the classical cosine similarity by mapping the observed words distribution into the latent semantic space, which leads to a generalized lexical cosine similarity measurement. Extensive experiments on benchmark dataset validate the effectiveness of our story co-segmentation approach. Besides, we specifically demonstrate the superior performance of the proposed WSCAG semantic similarity measure over other state-of-the-art semantic measures in story co-segmentation.}
}
@article{FALOMIR20153,
title = {Logics based on qualitative descriptors for scene understanding},
journal = {Neurocomputing},
volume = {161},
pages = {3-16},
year = {2015},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.01.074},
url = {https://www.sciencedirect.com/science/article/pii/S0925231215001587},
author = {Zoe Falomir and Ana-Maria Olteţeanu},
keywords = {Qualitative shape and colour, Topology, Location, Domain knowledge, Object detectors, Logics},
abstract = {An approach for scene understanding based on qualitative descriptors, domain knowledge and logics is proposed in this paper. Qualitative descriptors, qualitative models of shape, colour, topology and location are used for describing any object in the scene. Two kinds of domain knowledge are provided: (i) categorizations of objects according to their qualitative descriptors, and (ii) semantics for describing the affordances, mobility and other functional properties of target objects. First order logics are obtained for reasoning and scene understanding. Tests were carried out at the Interact@Cartesium scenario and promising results were obtained.}
}
@article{ELTON20219,
title = {Applying Deutsch’s concept of good explanations to artificial intelligence and neuroscience – An initial exploration},
journal = {Cognitive Systems Research},
volume = {67},
pages = {9-17},
year = {2021},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2020.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S138904172030108X},
author = {Daniel C. Elton},
keywords = {Deep learning, Artificial intelligence, Intelligence, Generalization, Extrapolation, Interpolation, Occam’s razor, Simplicity, Critical rationalism, Induction, Karl Popper},
abstract = {Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.}
}
@article{WINSTON201292,
title = {The next 50years: A personal view},
journal = {Biologically Inspired Cognitive Architectures},
volume = {1},
pages = {92-99},
year = {2012},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2012.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X12000035},
author = {Patrick Henry Winston},
keywords = {Biologically inspired cognitive models, Human intelligence, Evolution of intelligence, Inner language, Story understanding, Directed perception},
abstract = {I review history, starting with Turing’s seminal paper, reaching back ultimately to when our species started to outperform other primates, searching for the questions that will help us develop a computational account of human intelligence. I answer that the right questions are: What’s different between us and the other primates and what’s the same. I answer the what’s different question by saying that we became symbolic in a way that enabled story understanding, directed perception, and easy communication, and other species did not. I argue against Turing’s reasoning-centered suggestions, offering that reasoning is just a special case of story understanding. I answer the what’s the same question by noting that our brains are largely engineered in the same exotic way, with information flowing in all directions at once. By way of example, I illustrate how these answers can influence a research program, describing the Genesis system, a system that works with short summaries of stories, provided in English, together with low-level common-sense rules and higher-level concept patterns, likewise expressed in English. Genesis answers questions, notes abstract concepts such as revenge, tells stories in a listener-aware way, and fills in story gaps using precedents. I conclude by suggesting, optimistically, that a genuine computational theory of human intelligence will emerge in the next 50years if we stick to the right, biologically inspired questions, and work toward biologically informed models.}
}
@article{SILVA20147843,
title = {Building an Ontology for Intelligent Maintenance Systems and Spare Parts Supply Chain Integration},
journal = {IFAC Proceedings Volumes},
volume = {47},
number = {3},
pages = {7843-7848},
year = {2014},
note = {19th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20140824-6-ZA-1003.02217},
url = {https://www.sciencedirect.com/science/article/pii/S147466701642848X},
author = {Thiago Regal da Silva and Carlos Eduardo Pereira},
abstract = {Global competition has been leading to more complex production systems, in which satisfactory maintenance is crucial for the operations. The ability to properly forecast failures, provided by Intelligent Maintenance Systems (IMS) can avoid downtimes and provide a competitive advantage. Moreover, it can also enable more precise demand planning in Spare Parts Supply Chain (SPSC), resulting in the availability of parts and services when they are necessary in shop floor, avoiding breakdowns and production interruptions. The proper integration of IMS and SPSC is of utmost importance to achieve these results. Some of the challenges related to this integration refer to semantic differences between these areas with diverse concepts and vocabulary. This work intends to propose the building of an ontology to overcome these difficulties by providing a common vocabulary and proper semantic integration of the areas, as a basis for the construction of a future integration information system to integrate IMS and SPSC.}
}
@article{DEOLIVEIRA20181837,
title = {Contextual Knowledge to Enhance Workplace Hazard Recognition and Interpretation in a Cognitive Vision Platform},
journal = {Procedia Computer Science},
volume = {126},
pages = {1837-1846},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.093},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918313711},
author = {Caterine Silva {de Oliveira} and Cesar Sanin and Edward Szczerbicki},
keywords = {Context, Contextual Knowledge, CPS, SOEKS, DDNA, Hazard Control},
abstract = {The combination of vision and sensor data together with the resulting necessity for formal representations builds a central component of an autonomous Cyber Physical System for detection and tracking of laborers in workplaces environments. This system must be adaptable and perceive the environment as automatically as possible, performing in a variety of plants and scenes without the necessity of recoding the application for each specific use. But each recognition system has its own inherent limits, especially those which task is to work in unidentified environments and deal with unknown scenarios and specifications. The platform described in this paper takes this into account by connecting the probabilistic area of event detection with the logical area of formal reasoning in a Cognitive Vision Platform for Hazard Control (CVP-HC). In order to support formal reasoning, additional relational scene information is supplied to the recognition system. In this platform, the contextual knowledge is used to improve the recognition and interpretation of detected events. This relational data together with all collected information is represented explicitly as a Set of Experience Knowledge Structure (SOEKS), categorized and stored as a Decisional DNA (DDNA), a decisional safety fingerprint of a company. By these means, the systems assesses and addresses critical unsafe behaviors whilst gives support to an explicit long term culture change process. By the use of context the CVP-HC is capable adjust accordingly without the need of rewriting the application’s code every time conditions or specifications changes.}
}
@article{CAMBRIA2015443,
title = {An ELM-based model for affective analogical reasoning},
journal = {Neurocomputing},
volume = {149},
pages = {443-455},
year = {2015},
note = {Advances in neural networks Advances in Extreme Learning Machines},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2014.01.064},
url = {https://www.sciencedirect.com/science/article/pii/S0925231214011187},
author = {Erik Cambria and Paolo Gastaldo and Federica Bisio and Rodolfo Zunino},
keywords = {Artificial intelligence, Extreme learning machine, Analogical reasoning, Natural language processing, Opinion mining},
abstract = {Between the dawn of the Internet through year 2003, there were just a few dozens exabytes of information on the Web. Today, that much information is created weekly. The opportunity to capture the opinions of the general public about social events, political movements, company strategies, marketing campaigns, and product preferences has raised increasing interest both in the scientific community, for the exciting open challenges, and in the business world, for the remarkable fallouts in marketing and financial prediction. Keeping up with the ever-growing amount of unstructured information on the Web, however, is a formidable task and requires fast and efficient models for opinion mining. In this paper, we explore how the high generalization performance, low computational complexity, and fast learning speed of extreme learning machines can be exploited to perform analogical reasoning in a vector space model of affective common-sense knowledge. In particular, by enabling a fast reconfiguration of such a vector space, extreme learning machines allow the polarity associated with natural language concepts to be calculated in a more dynamic and accurate way and, hence, perform better concept-level sentiment analysis.}
}
@article{ALMOADHEN20141023,
title = {Automation in Handling Uncertainty in Semantic-knowledge based Robotic Task-planning by Using Markov Logic Networks},
journal = {Procedia Computer Science},
volume = {35},
pages = {1023-1032},
year = {2014},
note = {Knowledge-Based and Intelligent Information & Engineering Systems 18th Annual Conference, KES-2014 Gdynia, Poland, September 2014 Proceedings},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.08.188},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914011533},
author = {Ahmed Al-Moadhen and Michael Packianather and Rossi Setchi and Renxi Qiu},
keywords = {Task-Planning, Semantic-Knowledge Based, Planning Under Uncertainty, Markov Logic Networks},
abstract = {Generating plans in real world environments by mobile robot planner is a challenging task due to the uncertainty and environment dynamics. Therefore, task-planning should take in its consideration these issues when generating plans. Semantic knowledge domain has been proposed as a source of information for deriving implicit information and generating semantic plans. This paper extends the Semantic-Knowledge Based (SKB) plan generation to take into account the uncertainty in existing of objects, with their types and properties, and proposes a new approach to construct plans based on probabilistic values which are derived from Markov Logic Networks (MLN). An MLN module is established for probabilistic learning and inferencing together with semantic information to provide a basis for plausible learning and reasoning services in supporting of robot task-planning. In addition, an algorithm has been devised to construct MLN from semantic knowledge. By providing a means of modeling uncertainty in system architecture, task-planning serves as a supporting tool for robotic applications that can benefit from probabilistic inference within a semantic domain. This approach is illustrated using test scenarios run in a domestic environment using a mobile robot.}
}
@article{DERRAC201566,
title = {Inducing semantic relations from conceptual spaces: A data-driven approach to plausible reasoning},
journal = {Artificial Intelligence},
volume = {228},
pages = {66-94},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215001034},
author = {Joaquín Derrac and Steven Schockaert},
keywords = {Conceptual spaces, Dimensionality reduction, Qualitative spatial relations, Commonsense reasoning},
abstract = {Commonsense reasoning patterns such as interpolation and a fortiori inference have proven useful for dealing with gaps in structured knowledge bases. An important difficulty in applying these reasoning patterns in practice is that they rely on fine-grained knowledge of how different concepts and entities are semantically related. In this paper, we show how the required semantic relations can be learned from a large collection of text documents. To this end, we first induce a conceptual space from the text documents, using multi-dimensional scaling. We then rely on the key insight that the required semantic relations correspond to qualitative spatial relations in this conceptual space. Among others, in an entirely unsupervised way, we identify salient directions in the conceptual space which correspond to interpretable relative properties such as ‘more fruity than’ (in a space of wines), resulting in a symbolic and interpretable representation of the conceptual space. To evaluate the quality of our semantic relations, we show how they can be exploited by a number of commonsense reasoning based classifiers. We experimentally show that these classifiers can outperform standard approaches, while being able to provide intuitive explanations of classification decisions. A number of crowdsourcing experiments provide further insights into the nature of the extracted semantic relations.}
}
@article{BARTAK201754,
title = {Modeling and solving planning problems in tabled logic programming: Experience from the Cave Diving domain},
journal = {Science of Computer Programming},
volume = {147},
pages = {54-77},
year = {2017},
note = {Selected and Extended papers from the International Symposium on Principles and Practice of Declarative Programming 2015},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2017.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167642317300825},
author = {Roman Barták and Lukáš Chrpa and Agostino Dovier and Jindřich Vodrážka and Neng-Fa Zhou},
keywords = {Planning, Domain modeling, Logic programming, Tabling},
abstract = {Action planning deals with the problem of finding a sequence of actions transferring the world from a given state to a desired (goal) state. This problem is important in various areas such as robotics, manufacturing, transportation, autonomic computing, computer games, etc. Action planning is a form of a reachability problem in a huge state space so it is critical to efficiently represent world states and actions (transitions between states). In this paper we present a modeling framework for planning problems based on tabled logic programming that exploits a planner module in the Picat language. In particular, we suggest techniques for structured representation of states and for including control knowledge in the description of actions. We demonstrate these techniques using the complex planning domain Cave Diving from the International Planning Competition. Experimentally, we show properties of the model for different search approaches and we compare the performance of the proposed approach with state-of-the-art automated planners. The focus of this paper is on providing guidelines for manual modeling of planning domains rather than on automated reformulation of models.}
}
@article{SEKH2020107412,
title = {Can we automate diagrammatic reasoning?},
journal = {Pattern Recognition},
volume = {106},
pages = {107412},
year = {2020},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2020.107412},
url = {https://www.sciencedirect.com/science/article/pii/S0031320320302156},
author = {Arif Ahmed Sekh and Debi Prosad Dogra and Samarjit Kar and Partha Pratim Roy and Dilip K. Prasad},
keywords = {Abstract reasoning, Raven,s Progressive Matrices (RPM), Diagrammatic reasoning, Visual IQ test},
abstract = {Diagrammatic reasoning (DR) problems are well known. However, solving DR problems represented in 4 × 1 Raven’s Progressive Matrix (RPM) form using computer vision and pattern recognition has not yet been tried. Emergence of deep learning techniques aided by advanced computing can be exploited to solve such DR problems. In this paper, we propose a new learning framework by combining LSTM and Convolutional LSTM to solve 4 × 1 DR problems. Initially, the elementary geometrical shapes in such problems are detected using a typical CNN-based detector. Next, relations of various shapes are analyzed and a high-level feature set is produced and processed in the LSTM framework. A new 4 × 1 DR dataset has been prepared and made available to the research community. We believe, it will be helpful in advancing this research further. We have compared our method with some of the existing frameworks that can be used for solving RPM-guided DR problems. We have recorded 18–20% increase in the average prediction accuracy as compared to the prior frameworks when applied to RPM-guided DR problems. We believe the CV research community will be interested to carry out similar research, particularly to investigate the feasibility of solving other types of known DR problems.}
}
@article{NOVAK201225,
title = {Reasoning about mathematical fuzzy logic and its future},
journal = {Fuzzy Sets and Systems},
volume = {192},
pages = {25-44},
year = {2012},
note = {Fuzzy Set Theory — Where Do We Stand and Where Do We Go?},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2010.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S0165011410003684},
author = {Vilém Novák},
keywords = {Fuzzy logic in narrow sense, Fuzzy logic in broader sense, Evaluative linguistic expressions, Intermediate quantifiers, Vagueness, Fuzzy/linguistic IF–THEN rules, Perception-based logical deduction, Approximate reasoning, Fuzzy mathematics, Linguistic semantics},
abstract = {This paper is devoted to reasoning about fuzzy logic which is based on various personal observations of the author. Our goal is to think of the state of the art in mathematical fuzzy logic (MFL) and to outline some of the tasks on which, in the author's opinion, MFL should focus in the future. In our discussion, we will mention not only the basic theory, but also its extension called fuzzy logic in broader sense (FLb). The paradigm of the latter is to be the logic of natural human reasoning, whose most essential characteristic is the use of natural language. Besides brief description of FLb, we will also mention some of its applications. On the basis of that, we will ponder on other possible directions for research, namely the possibility of using FLn as a metatheory of fuzzy mathematics, as a proper tool for modeling of the main manifestations of the phenomenon of vagueness, and as a reasonable tool for developing models of linguistic semantics.}
}